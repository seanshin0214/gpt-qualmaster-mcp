"""
GPT QualMaster MCP Server
=========================
AI-Powered Qualitative Research & Conceptual Paper Writing Assistant

ì›ë³¸: qualmaster-mcp-server (Claude MCP - TypeScript)
GPT Desktopìš© Python FastAPI í¬íŒ…
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
import asyncio
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# ChromaDB (optional - graceful fallback)
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMER_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMER_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ChromaDB Configuration - PersistentClient (Local Storage)
BASE_DIR = Path(__file__).parent
CHROMA_PATH = BASE_DIR / "data" / "chroma_db"


class QualMasterVectorStore:
    """RAG ë²¡í„° ìŠ¤í† ì–´ - ChromaDB PersistentClient ê¸°ë°˜"""

    def __init__(self, chroma_path: str):
        self._client = None
        self._collection = None
        self._encoder = None
        self._chroma_path = chroma_path

    @property
    def encoder(self):
        if self._encoder is None and SENTENCE_TRANSFORMER_AVAILABLE:
            self._encoder = SentenceTransformer('all-MiniLM-L6-v2')
        return self._encoder

    @property
    def collection(self):
        if self._collection is None:
            self._client = chromadb.PersistentClient(path=self._chroma_path)
            self._collection = self._client.get_collection("qualmaster_knowledge")
            logger.info(f"Vector store loaded: {self._collection.count()} documents")
        return self._collection

    def search(self, query: str, n_results: int = 5, category: str = None) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰"""
        try:
            if not self.encoder:
                return []
            query_embedding = self.encoder.encode([query]).tolist()

            where_filter = None
            if category:
                where_filter = {"category": category}

            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=n_results,
                where=where_filter
            )

            formatted = []
            for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
                formatted.append({
                    "content": doc,
                    "title": meta.get("title", ""),
                    "source": meta.get("source", ""),
                    "category": meta.get("category", ""),
                    "rank": i + 1
                })
            return formatted
        except Exception as e:
            logger.error(f"Vector search error: {e}")
            return []

    def get_stats(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        try:
            return {
                "total_documents": self.collection.count(),
                "status": "connected"
            }
        except:
            return {"status": "disconnected"}


# Global vector store
vector_store: Optional[QualMasterVectorStore] = None


def init_chromadb():
    """Initialize ChromaDB PersistentClient"""
    global vector_store
    if not CHROMADB_AVAILABLE:
        logger.warning("ChromaDB not installed - RAG search disabled")
        return False
    try:
        chroma_path = str(CHROMA_PATH)
        if not CHROMA_PATH.exists():
            logger.warning(f"ChromaDB path not found: {chroma_path}")
            logger.warning("Run 'python init_vectordb.py' to initialize the vector database")
            return False

        vector_store = QualMasterVectorStore(chroma_path)
        stats = vector_store.get_stats()
        logger.info(f"ChromaDB PersistentClient connected: {stats['total_documents']} documents")
        return True
    except Exception as e:
        logger.warning(f"ChromaDB connection failed: {e}")
        vector_store = None
        return False

def safe_decode(text):
    """Safely decode text to UTF-8"""
    if isinstance(text, bytes):
        try:
            return text.decode('utf-8')
        except UnicodeDecodeError:
            try:
                return text.decode('cp949')
            except:
                return text.decode('utf-8', errors='ignore')
    return str(text) if text else ""

def search_chromadb(query: str, n_results: int = 5, category: str = None) -> List[dict]:
    """Search ChromaDB for relevant documents using PersistentClient"""
    if not vector_store:
        return []

    try:
        results = vector_store.search(query, n_results, category)
        return results
    except Exception as e:
        logger.debug(f"Vector search failed: {e}")
        return []


# ============================================================================
# Knowledge Base (Embedded - No RAG dependency)
# ============================================================================

PARADIGMS = {
    "positivism": {
        "name": "ì‹¤ì¦ì£¼ì˜ (Positivism)",
        "ontology": "ë‹¨ì¼í•œ ê°ê´€ì  ì‹¤ì¬ ì¡´ì¬",
        "epistemology": "ì—°êµ¬ìì™€ ëŒ€ìƒì˜ ë¶„ë¦¬, ê°ê´€ì  ê´€ì°° ê°€ëŠ¥",
        "methodology": "ì‹¤í—˜, ê²€ì¦, ê°€ì„¤ ê²€ì¦",
        "quality_criteria": ["ë‚´ì íƒ€ë‹¹ë„", "ì™¸ì íƒ€ë‹¹ë„", "ì‹ ë¢°ë„", "ê°ê´€ì„±"],
        "key_scholars": ["Auguste Comte", "Ã‰mile Durkheim"],
        "limitations": "ì¸ê°„ ê²½í—˜ì˜ ë³µì¡ì„±ê³¼ ë§¥ë½ ë¬´ì‹œ ê°€ëŠ¥ì„±"
    },
    "postpositivism": {
        "name": "í›„ê¸°ì‹¤ì¦ì£¼ì˜ (Post-positivism)",
        "ontology": "ì‹¤ì¬ ì¡´ì¬í•˜ë‚˜ ì™„ì „íˆ íŒŒì•… ë¶ˆê°€ (Critical Realism)",
        "epistemology": "ê°ê´€ì„±ì€ ëª©í‘œì´ë‚˜ í¸í–¥ ì¸ì •, ë°˜ì¦ì£¼ì˜",
        "methodology": "ìˆ˜ì •ëœ ì‹¤í—˜, ì‚¼ê°ê²€ì¦, ì¤€ì‹¤í—˜ì„¤ê³„",
        "quality_criteria": ["ë‚´ì íƒ€ë‹¹ë„", "ì™¸ì íƒ€ë‹¹ë„", "ì‹ ë¢°ë„", "ê°ê´€ì„± (ìˆ˜ì •ë¨)"],
        "key_scholars": ["Karl Popper", "Thomas Kuhn"],
        "limitations": "ì—¬ì „íˆ ê°ê´€ì£¼ì˜ì  ê°€ì • ìœ ì§€"
    },
    "critical_theory": {
        "name": "ë¹„íŒì´ë¡  (Critical Theory)",
        "ontology": "ì—­ì‚¬ì ìœ¼ë¡œ êµ¬ì„±ëœ ì‹¤ì¬, ê¶Œë ¥ê´€ê³„ì— ì˜í•´ í˜•ì„±",
        "epistemology": "ì—°êµ¬ì-ëŒ€ìƒ ìƒí˜¸ì‘ìš©, ê°€ì¹˜ ê°œì… ì¸ì •",
        "methodology": "ì´ë°ì˜¬ë¡œê¸° ë¹„íŒ, ì°¸ì—¬ì  í–‰ë™ì—°êµ¬, ë‹´ë¡ ë¶„ì„",
        "quality_criteria": ["ì—­ì‚¬ì„±", "ì¹¨ì‹ì„±", "ì´‰ë§¤ì  íƒ€ë‹¹ì„±", "í•´ë°©ì  íƒ€ë‹¹ì„±"],
        "key_scholars": ["Theodor Adorno", "JÃ¼rgen Habermas", "Paulo Freire"],
        "limitations": "ì—°êµ¬ìì˜ ì •ì¹˜ì  ì…ì¥ì´ ì—°êµ¬ì— ê³¼ë„í•˜ê²Œ ê°œì… ê°€ëŠ¥"
    },
    "constructivism": {
        "name": "êµ¬ì„±ì£¼ì˜ (Constructivism)",
        "ontology": "ë‹¤ì¤‘ ì‹¤ì¬, ì‚¬íšŒì ìœ¼ë¡œ êµ¬ì„±ë¨",
        "epistemology": "ì—°êµ¬ì-ì°¸ì—¬ì ê³µë™ êµ¬ì„±, í•´ì„í•™ì  ì´í•´",
        "methodology": "í•´ì„í•™, ê·¼ê±°ì´ë¡ , í˜„ìƒí•™, ë‚´ëŸ¬í‹°ë¸Œ",
        "quality_criteria": ["ì‹ ë¢°ì„±", "ì „ì´ê°€ëŠ¥ì„±", "ì˜ì¡´ì„±", "í™•ì¦ì„±"],
        "key_scholars": ["Egon Guba", "Yvonna Lincoln", "John Creswell"],
        "limitations": "ìƒëŒ€ì£¼ì˜ì  ì…ì¥ìœ¼ë¡œ ì¸í•œ ì¼ë°˜í™” ì–´ë ¤ì›€"
    }
}

TRADITIONS = {
    "phenomenology": {
        "name": "í˜„ìƒí•™ (Phenomenology)",
        "focus": "ì²´í—˜ì˜ ë³¸ì§ˆ (essence of lived experience)",
        "data_collection": "ì‹¬ì¸µ ì¸í„°ë·°, ì°¸ì—¬ ê´€ì°°",
        "analysis": "í˜„ìƒí•™ì  í™˜ì›, ë³¸ì§ˆ ì§ê´€, ì˜ë¯¸ ë‹¨ìœ„ ë¶„ì„",
        "sample_size": "3-10ëª…",
        "key_scholars": ["Husserl", "Heidegger", "Merleau-Ponty", "van Manen"],
        "variants": {
            "husserlian": "ê¸°ìˆ ì  í˜„ìƒí•™ - ë³¸ì§ˆ ê¸°ìˆ ì— ì§‘ì¤‘",
            "heideggerian": "í•´ì„í•™ì  í˜„ìƒí•™ - ì¡´ì¬ë¡ ì  í•´ì„",
            "ipa": "í•´ì„í•™ì  í˜„ìƒí•™ì  ë¶„ì„ - ì°¸ì—¬ìì˜ ì˜ë¯¸ ë§Œë“¤ê¸°"
        }
    },
    "grounded_theory": {
        "name": "ê·¼ê±°ì´ë¡  (Grounded Theory)",
        "focus": "í˜„ìƒì„ ì„¤ëª…í•˜ëŠ” ì´ë¡  ê°œë°œ",
        "data_collection": "ì¸í„°ë·°, ê´€ì°°, ë¬¸ì„œ (ì´ë¡ ì  í‘œì§‘)",
        "analysis": "ê°œë°©ì½”ë”©â†’ì¶•ì½”ë”©â†’ì„ íƒì½”ë”©, ì§€ì†ì  ë¹„êµ",
        "sample_size": "20-30ëª… (í¬í™” ì‹œì ê¹Œì§€)",
        "key_scholars": ["Glaser", "Strauss", "Corbin", "Charmaz"],
        "variants": {
            "glaserian": "ì›ì „ GT - ì¶œí˜„, ì—°êµ¬ì ì¤‘ë¦½ì„± ê°•ì¡°",
            "straussian": "ì²´ê³„ì  GT - ë¶„ì„ ì ˆì°¨ ëª…í™•í™”",
            "charmaz": "êµ¬ì„±ì£¼ì˜ GT - ì—°êµ¬ìì˜ í•´ì„ì  ì—­í•  ì¸ì •"
        }
    },
    "ethnography": {
        "name": "ë¬¸í™”ê¸°ìˆ ì§€ (Ethnography)",
        "focus": "ë¬¸í™” ê³µìœ  ì§‘ë‹¨ì˜ íŒ¨í„´",
        "data_collection": "ì¥ê¸°ê°„ í˜„ì¥ì—°êµ¬, ì°¸ì—¬ê´€ì°°, ì¸í„°ë·°",
        "analysis": "ê¸°ìˆ , ë¶„ì„, í•´ì„",
        "sample_size": "ë¬¸í™” ê³µìœ  ì§‘ë‹¨ ì „ì²´",
        "key_scholars": ["Clifford Geertz", "BronisÅ‚aw Malinowski"],
        "variants": {
            "classical": "ì „í†µì  ë¯¼ì¡±ì§€ - ì™¸ë¶€ì ê´€ì ",
            "autoethnography": "ìê¸° ë¬¸í™”ê¸°ìˆ ì§€ - ì—°êµ¬ì ê²½í—˜ ì¤‘ì‹¬",
            "critical": "ë¹„íŒì  ë¯¼ì¡±ì§€ - ê¶Œë ¥ê´€ê³„ ë¶„ì„"
        }
    },
    "narrative": {
        "name": "ë‚´ëŸ¬í‹°ë¸Œ íƒêµ¬ (Narrative Inquiry)",
        "focus": "ê°œì¸ì˜ ì´ì•¼ê¸°, ìƒì• ì‚¬",
        "data_collection": "ìƒì• ì‚¬ ì¸í„°ë·°, ì €ë„, ë¬¸ì„œ",
        "analysis": "ì¤„ê±°ë¦¬ ë¶„ì„, ì£¼ì œ ë¶„ì„, êµ¬ì¡° ë¶„ì„",
        "sample_size": "1-5ëª…",
        "key_scholars": ["D. Jean Clandinin", "F. Michael Connelly"],
        "variants": {
            "biographical": "ì „ê¸°ì  ì ‘ê·¼",
            "life_history": "ìƒì• ì‚¬ ì—°êµ¬",
            "oral_history": "êµ¬ìˆ ì‚¬"
        }
    },
    "case_study": {
        "name": "ì‚¬ë¡€ì—°êµ¬ (Case Study)",
        "focus": "ì‚¬ë¡€ì˜ ì‹¬ì¸µì  ì´í•´ (ê²½ê³„ ì§€ì–´ì§„ ì²´ê³„)",
        "data_collection": "ì¸í„°ë·°, ê´€ì°°, ë¬¸ì„œ, ì•„ì¹´ì´ë¸Œ",
        "analysis": "ì‚¬ë¡€ ë‚´ ë¶„ì„, êµì°¨ ì‚¬ë¡€ ë¶„ì„",
        "sample_size": "1-5 ì‚¬ë¡€",
        "key_scholars": ["Robert Yin", "Robert Stake", "Kathleen Eisenhardt"],
        "variants": {
            "intrinsic": "ë³¸ì§ˆì  - ì‚¬ë¡€ ìì²´ê°€ ëª©ì ",
            "instrumental": "ë„êµ¬ì  - ì‚¬ë¡€ë¥¼ í†µí•œ ì´ë¡  ë°œì „",
            "collective": "ì§‘í•©ì  - ë‹¤ì¤‘ ì‚¬ë¡€ ë¹„êµ"
        }
    }
}

CODING_TYPES = {
    "open_coding": {
        "name": "ê°œë°©ì½”ë”© (Open Coding)",
        "description": "ë°ì´í„°ë¥¼ ê°œë…ìœ¼ë¡œ ë¶„í•´í•˜ê³  ëª…ëª…í•˜ëŠ” ê³¼ì •",
        "process": ["ë¼ì¸ë³„/ë¬¸ì¥ë³„/ë‹¨ë½ë³„ ë¶„ì„", "ê°œë… ëª…ëª…", "ë²”ì£¼í™”"],
        "output": "ê°œë… ëª©ë¡, ì´ˆê¸° ë²”ì£¼"
    },
    "axial_coding": {
        "name": "ì¶•ì½”ë”© (Axial Coding)",
        "description": "ë²”ì£¼ ê°„ ê´€ê³„ êµ¬ì¡°í™”",
        "paradigm_model": {
            "causal_conditions": "ì¸ê³¼ì  ì¡°ê±´",
            "phenomenon": "í˜„ìƒ",
            "context": "ë§¥ë½ì  ì¡°ê±´",
            "intervening": "ì¤‘ì¬ì  ì¡°ê±´",
            "strategies": "ì‘ìš©/ìƒí˜¸ì‘ìš© ì „ëµ",
            "consequences": "ê²°ê³¼"
        },
        "output": "íŒ¨ëŸ¬ë‹¤ì„ ëª¨í˜•, ë²”ì£¼ ê´€ê³„ë„"
    },
    "selective_coding": {
        "name": "ì„ íƒì½”ë”© (Selective Coding)",
        "description": "í•µì‹¬ë²”ì£¼ ì„ ì • ë° ì´ë¡  í†µí•©",
        "process": ["ìŠ¤í† ë¦¬ë¼ì¸ ì‘ì„±", "í•µì‹¬ë²”ì£¼ í™•ì¸", "ì´ë¡ ì  í‹€ ì™„ì„±"],
        "output": "ì´ë¡ ì  ëª¨í˜•, ëª…ì œ"
    },
    "invivo_coding": {
        "name": "ì¸ë¹„ë³´ ì½”ë”© (In Vivo Coding)",
        "description": "ì°¸ì—¬ìì˜ ì‹¤ì œ ì–¸ì–´ë¥¼ ì½”ë“œë¡œ ì‚¬ìš©",
        "purpose": "ì°¸ì—¬ì ê´€ì  ë³´ì¡´",
        "example": '"ê·¸ëƒ¥ ë²„í‹°ëŠ” ê±°ì£ " â†’ ë²„í‹°ê¸°'
    },
    "thematic_analysis": {
        "name": "ì£¼ì œë¶„ì„ (Thematic Analysis)",
        "process": [
            "1. ë°ì´í„° ì¹œìˆ™í•´ì§€ê¸°",
            "2. ì´ˆê¸° ì½”ë“œ ìƒì„±",
            "3. ì£¼ì œ íƒìƒ‰",
            "4. ì£¼ì œ ê²€í† ",
            "5. ì£¼ì œ ì •ì˜ ë° ëª…ëª…",
            "6. ë³´ê³ ì„œ ì‘ì„±"
        ],
        "key_scholars": ["Braun & Clarke (2006)"]
    }
}

QUALITY_CRITERIA = {
    "trustworthiness": {
        "credibility": {
            "name": "ì‹ ë¢°ì„± (Credibility)",
            "quantitative_equivalent": "ë‚´ì  íƒ€ë‹¹ë„",
            "strategies": [
                "ì¥ê¸°ê°„ ì°¸ì—¬ (Prolonged engagement)",
                "ì‚¼ê°ê²€ì¦ (Triangulation)",
                "ë™ë£Œ ê²€í†  (Peer debriefing)",
                "ì°¸ì—¬ì í™•ì¸ (Member checking)",
                "ë¶€ì •ì‚¬ë¡€ ë¶„ì„ (Negative case analysis)"
            ]
        },
        "transferability": {
            "name": "ì „ì´ê°€ëŠ¥ì„± (Transferability)",
            "quantitative_equivalent": "ì™¸ì  íƒ€ë‹¹ë„",
            "strategies": [
                "í’ë¶€í•œ ê¸°ìˆ  (Thick description)",
                "ë§¥ë½ ìƒì„¸ ê¸°ìˆ ",
                "ì°¸ì—¬ì íŠ¹ì„± ëª…ì‹œ"
            ]
        },
        "dependability": {
            "name": "ì˜ì¡´ì„± (Dependability)",
            "quantitative_equivalent": "ì‹ ë¢°ë„",
            "strategies": [
                "ê°ì‚¬ ì¶”ì  (Audit trail)",
                "íƒêµ¬ ê³¼ì • ë¬¸ì„œí™”",
                "ì—°êµ¬ ì¼ì§€ (Reflexive journal)"
            ]
        },
        "confirmability": {
            "name": "í™•ì¦ì„± (Confirmability)",
            "quantitative_equivalent": "ê°ê´€ì„±",
            "strategies": [
                "ê°ì‚¬ ì¶”ì ",
                "ì‚¼ê°ê²€ì¦",
                "ë°˜ì„±ì  ì„±ì°° (Reflexivity)"
            ]
        }
    }
}

JOURNALS = {
    "amr": {
        "name": "Academy of Management Review",
        "focus": "ì´ë¡  ê°œë°œ, ê°œë… ë…¼ë¬¸",
        "style": "ê°€ì„¤ ì—†ìŒ, ëª…ì œ ì¤‘ì‹¬",
        "key_sections": ["Introduction", "Theoretical Background", "Theory Development", "Discussion"],
        "common_rejections": [
            "So What? - ê¸°ì—¬ê°€ ëª…í™•í•˜ì§€ ì•ŠìŒ",
            "Old Wine - ê¸°ì¡´ ì´ë¡ ì˜ ì¬í¬ì¥",
            "Logic Gaps - ë…¼ë¦¬ì  ë¹„ì•½",
            "Construct Confusion - ê°œë… í˜¼ë€"
        ],
        "tips": [
            "ì²« ë¬¸ë‹¨ì—ì„œ í¥ë¯¸ë¡œìš´ Puzzle ì œì‹œ",
            "ê¸°ì¡´ ì´ë¡ ì˜ í•œê³„ë¥¼ ëª…í™•íˆ",
            "ìƒˆë¡œìš´ ê°œë…/ë©”ì»¤ë‹ˆì¦˜ ì œì•ˆ",
            "ê²½ìŸ ê°€ì„¤ê³¼ì˜ ë¹„êµ",
            "ê²½ê³„ì¡°ê±´ ëª…ì‹œ"
        ]
    },
    "asq": {
        "name": "Administrative Science Quarterly",
        "focus": "ê²½í—˜ì  ì—°êµ¬ + ê°•í•œ ì´ë¡ ",
        "style": "ì •ì„±/ì •ëŸ‰ ëª¨ë‘ ê°€ëŠ¥",
        "requirements": [
            "ìƒˆë¡œìš´ ì´ë¡ ì  í†µì°°",
            "ì² ì €í•œ ë°©ë²•ë¡ ",
            "í’ë¶€í•œ ë°ì´í„°"
        ]
    }
}

REJECTION_PATTERNS = {
    "so_what": {
        "name": "So What? ë¬¸ì œ",
        "symptoms": ["ê¸°ì—¬ ë¶ˆëª…í™•", "ì‹¤ë¬´ì  í•¨ì˜ ë¶€ì¡±", "ì´ë¡ ì  ì¤‘ìš”ì„± ì„¤ëª… ë¶€ì¡±"],
        "solutions": [
            "ì„œë¡ ì—ì„œ ì—°êµ¬ ì§ˆë¬¸ì˜ ì¤‘ìš”ì„± ê°•ì¡°",
            "ê¸°ì¡´ ì´ë¡ /ì‹¤ë¬´ì˜ êµ¬ì²´ì  í•œê³„ ì œì‹œ",
            "ì—°êµ¬ ê²°ê³¼ì˜ ì´ë¡ ì /ì‹¤ë¬´ì  í•¨ì˜ ëª…í™•í™”"
        ]
    },
    "old_wine": {
        "name": "Old Wine in New Bottles ë¬¸ì œ",
        "symptoms": ["ê¸°ì¡´ ì—°êµ¬ì™€ ì°¨ë³„ì  ë¶ˆëª…í™•", "ìƒˆë¡œìš´ í†µì°° ë¶€ì¡±"],
        "solutions": [
            "ê¸°ì¡´ ì—°êµ¬ì™€ì˜ ëª…í™•í•œ ì°¨ë³„ì  ì œì‹œ",
            "ìƒˆë¡œìš´ ë§¥ë½/ì¡°ê±´ì—ì„œì˜ ì ìš©",
            "ê¸°ì¡´ ì´ë¡ ì˜ ê²½ê³„ì¡°ê±´ íƒìƒ‰"
        ]
    },
    "logic_gap": {
        "name": "ë…¼ë¦¬ì  ë¹„ì•½ ë¬¸ì œ",
        "symptoms": ["ì¶”ë¡  ê³¼ì • ë¶ˆì™„ì „", "ë°ì´í„°-ê²°ë¡  ì—°ê²° ì•½í•¨"],
        "solutions": [
            "ë‹¨ê³„ë³„ ë…¼ë¦¬ ì „ê°œ",
            "ëŒ€ì•ˆ ì„¤ëª… ê³ ë ¤",
            "ì¶”ë¡ ì˜ ê·¼ê±° ëª…ì‹œ"
        ]
    }
}


# ============================================================================
# MCP Tools Definition
# ============================================================================

SERVER_INFO = {
    "name": "gpt-qualmaster-mcp",
    "version": "1.0.0",
    "description": "AI-Powered Qualitative Research & Conceptual Paper Writing Assistant for GPT Desktop"
}

TOOLS = [
    {
        "name": "search_knowledge",
        "description": """ì§ˆì ì—°êµ¬ ë° ê°œë…ë…¼ë¬¸ ì‘ì„± ì§€ì‹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

ê²€ìƒ‰ ê°€ëŠ¥í•œ ì£¼ì œ:
- ì—°êµ¬ íŒ¨ëŸ¬ë‹¤ì„ (ì‹¤ì¦ì£¼ì˜, êµ¬ì„±ì£¼ì˜ ë“±)
- ì§ˆì ì—°êµ¬ ì „í†µ (í˜„ìƒí•™, ê·¼ê±°ì´ë¡ , ë¬¸í™”ê¸°ìˆ ì§€ ë“±)
- ì½”ë”© ë°©ë²• (ê°œë°©ì½”ë”©, ì¶•ì½”ë”©, ì£¼ì œë¶„ì„)
- í’ˆì§ˆ ê¸°ì¤€ (ì‹ ë¢°ì„±, ì „ì´ê°€ëŠ¥ì„±)
- ì €ë„ íˆ¬ê³  (AMR, ASQ)
- R&R ì „ëµ (ë¦¬ì ì…˜ íŒ¨í„´, ìˆ˜ì • ê°€ì´ë“œ)
        """,
        "inputSchema": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "ê²€ìƒ‰í•  ì£¼ì œ ë˜ëŠ” ì§ˆë¬¸"},
                "category": {
                    "type": "string",
                    "enum": ["paradigms", "traditions", "coding", "quality", "journals", "rejection"],
                    "description": "ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬ (ì„ íƒì‚¬í•­)"
                }
            },
            "required": ["query"]
        }
    },
    {
        "name": "get_paradigm",
        "description": "íŠ¹ì • ì—°êµ¬ íŒ¨ëŸ¬ë‹¤ì„ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "paradigm": {
                    "type": "string",
                    "enum": ["positivism", "postpositivism", "critical_theory", "constructivism"],
                    "description": "íŒ¨ëŸ¬ë‹¤ì„ ì´ë¦„"
                }
            },
            "required": ["paradigm"]
        }
    },
    {
        "name": "get_tradition",
        "description": "íŠ¹ì • ì§ˆì ì—°êµ¬ ì „í†µì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "tradition": {
                    "type": "string",
                    "enum": ["phenomenology", "grounded_theory", "ethnography", "narrative", "case_study"],
                    "description": "ì§ˆì ì—°êµ¬ ì „í†µ"
                }
            },
            "required": ["tradition"]
        }
    },
    {
        "name": "suggest_methodology",
        "description": "ì—°êµ¬ì§ˆë¬¸ì— ì í•©í•œ ë°©ë²•ë¡ ì„ ì¶”ì²œí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "research_question": {"type": "string", "description": "ì—°êµ¬ì§ˆë¬¸"},
                "focus": {
                    "type": "string",
                    "enum": ["experience", "theory_building", "culture", "story", "case"],
                    "description": "ì—°êµ¬ ì´ˆì "
                }
            },
            "required": ["research_question"]
        }
    },
    {
        "name": "get_coding_guide",
        "description": "ì½”ë”© ë°©ë²•ì— ëŒ€í•œ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "coding_type": {
                    "type": "string",
                    "enum": ["open_coding", "axial_coding", "selective_coding", "invivo_coding", "thematic_analysis"],
                    "description": "ì½”ë”© ìœ í˜•"
                }
            },
            "required": ["coding_type"]
        }
    },
    {
        "name": "assess_quality",
        "description": """ì§ˆì ì—°êµ¬ì˜ í’ˆì§ˆì„ Lincoln & Guba + Tracy ê¸°ì¤€ìœ¼ë¡œ ì‹¤ì œ í‰ê°€í•©ë‹ˆë‹¤.

ì—°êµ¬ ì„¤ëª…ê³¼ ì‚¬ìš© ì „ëµì„ ì…ë ¥í•˜ë©´ 100ì  ë§Œì ìœ¼ë¡œ ì ìˆ˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.
- Lincoln & Guba: ì‹ ë¹™ì„±, ì „ì´ê°€ëŠ¥ì„±, ì˜ì¡´ê°€ëŠ¥ì„±, í™•ì¸ê°€ëŠ¥ì„±
- Tracy: ê°€ì¹˜ìˆëŠ” ì£¼ì œ, í’ë¶€í•œ ì—„ê²©ì„±, ì„±ì‹¤ì„±, ì‹ ë¹™ì„±, ê³µëª…, ì˜ë¯¸ìˆëŠ” ê¸°ì—¬, ìœ¤ë¦¬ì„±, ì˜ë¯¸ìˆëŠ” ì¼ê´€ì„±""",
        "inputSchema": {
            "type": "object",
            "properties": {
                "research_description": {
                    "type": "string",
                    "description": "ì—°êµ¬ ì„¤ëª… (ë°©ë²•ë¡ , ë°ì´í„° ìˆ˜ì§‘, ë¶„ì„ ì ˆì°¨ ë“±)"
                },
                "strategies_used": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ì‚¬ìš©í•œ í’ˆì§ˆ ì „ëµ ëª©ë¡ (ì˜ˆ: ì‚¼ê°ê²€ì¦, ì°¸ì—¬ìí™•ì¸, ê°ì‚¬ì¶”ì  ë“±)"
                },
                "criteria": {
                    "type": "string",
                    "enum": ["lincoln_guba", "tracy", "all"],
                    "description": "í‰ê°€ ê¸°ì¤€ (ê¸°ë³¸ê°’: all)"
                }
            },
            "required": ["research_description"]
        }
    },
    {
        "name": "get_journal_guide",
        "description": "ì €ë„ íˆ¬ê³  ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "journal": {
                    "type": "string",
                    "enum": ["amr", "asq"],
                    "description": "ì €ë„ ì´ë¦„"
                }
            },
            "required": ["journal"]
        }
    },
    {
        "name": "diagnose_rejection",
        "description": "ë¦¬ì ì…˜ íŒ¨í„´ì„ ì§„ë‹¨í•˜ê³  ëŒ€ì‘ ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "rejection_type": {
                    "type": "string",
                    "enum": ["so_what", "old_wine", "logic_gap"],
                    "description": "ë¦¬ì ì…˜ ìœ í˜•"
                }
            },
            "required": ["rejection_type"]
        }
    },
    {
        "name": "conceptualize_idea",
        "description": "ì—°êµ¬ ì•„ì´ë””ì–´ë¥¼ ê°œë…í™”í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "idea": {"type": "string", "description": "ì—°êµ¬ ì•„ì´ë””ì–´"},
                "field": {"type": "string", "description": "ì—°êµ¬ ë¶„ì•¼"}
            },
            "required": ["idea"]
        }
    },
    {
        "name": "develop_proposition",
        "description": "ì´ë¡ ì  ëª…ì œ ê°œë°œì„ ì§€ì›í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "concept_a": {"type": "string", "description": "ê°œë… A"},
                "concept_b": {"type": "string", "description": "ê°œë… B"},
                "relationship": {
                    "type": "string",
                    "enum": ["positive", "negative", "moderation", "mediation"],
                    "description": "ê´€ê³„ ìœ í˜•"
                }
            },
            "required": ["concept_a", "concept_b"]
        }
    },
    {
        "name": "review_paper",
        "description": "ë…¼ë¬¸ ì´ˆì•ˆì„ ë¦¬ë·°í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "paper_section": {
                    "type": "string",
                    "enum": ["introduction", "literature", "method", "findings", "discussion"],
                    "description": "ê²€í† í•  ì„¹ì…˜"
                },
                "content": {"type": "string", "description": "ê²€í† í•  ë‚´ìš©"}
            },
            "required": ["paper_section", "content"]
        }
    },
    {
        "name": "guide_revision",
        "description": "R&R ìˆ˜ì • ì „ëµì„ ê°€ì´ë“œí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "reviewer_comment": {"type": "string", "description": "ë¦¬ë·°ì–´ ì½”ë©˜íŠ¸"},
                "comment_type": {
                    "type": "string",
                    "enum": ["major", "minor", "clarification"],
                    "description": "ì½”ë©˜íŠ¸ ìœ í˜•"
                }
            },
            "required": ["reviewer_comment"]
        }
    }
]


# ============================================================================
# Tool Handlers
# ============================================================================

def handle_search_knowledge(args: dict) -> str:
    """ì§€ì‹ ê²€ìƒ‰ - ë‚´ì¥ ì§€ì‹ + ChromaDB RAG í†µí•©"""
    query = args.get("query", "").lower()
    original_query = args.get("query", "")
    category = args.get("category")

    results = []

    # 1. ë‚´ì¥ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰
    # íŒ¨ëŸ¬ë‹¤ì„ ê²€ìƒ‰
    if not category or category == "paradigms":
        for key, p in PARADIGMS.items():
            if query in key or query in p["name"].lower() or query in str(p).lower():
                results.append(f"**{p['name']}**\n- ì¡´ì¬ë¡ : {p['ontology']}\n- ì¸ì‹ë¡ : {p['epistemology']}")

    # ì „í†µ ê²€ìƒ‰
    if not category or category == "traditions":
        for key, t in TRADITIONS.items():
            if query in key or query in t["name"].lower() or query in str(t).lower():
                results.append(f"**{t['name']}**\n- ì´ˆì : {t['focus']}\n- ë¶„ì„: {t['analysis']}")

    # ì½”ë”© ê²€ìƒ‰
    if not category or category == "coding":
        for key, c in CODING_TYPES.items():
            if query in key or query in c["name"].lower() or query in str(c).lower():
                results.append(f"**{c['name']}**\n- {c['description']}")

    # ì €ë„ ê²€ìƒ‰
    if not category or category == "journals":
        for key, j in JOURNALS.items():
            if query in key or query in j["name"].lower():
                results.append(f"**{j['name']}**\n- ì´ˆì : {j['focus']}")

    # ë¦¬ì ì…˜ ê²€ìƒ‰
    if not category or category == "rejection":
        for key, r in REJECTION_PATTERNS.items():
            if query in key or query in r["name"].lower():
                results.append(f"**{r['name']}**\n- ì¦ìƒ: {', '.join(r['symptoms'])}")

    # 2. ChromaDB RAG ê²€ìƒ‰ (ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸)
    rag_results = search_chromadb(original_query, n_results=5)
    rag_section = ""
    if rag_results:
        rag_section = "\n\n---\n\n## ğŸ“š RAG ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼\n\n"
        for i, r in enumerate(rag_results[:3], 1):
            content_preview = r["content"][:500] + "..." if len(r["content"]) > 500 else r["content"]
            title = r.get("title", "")
            rag_section += f"### {i}. {title}\n{content_preview}\n\n"

    if results or rag_results:
        output = f"## '{original_query}' ê²€ìƒ‰ ê²°ê³¼\n\n"
        if results:
            output += "\n\n---\n\n".join(results)
        output += rag_section
        return output
    else:
        return f"'{original_query}'ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: paradigms, traditions, coding, quality, journals, rejection\n\nğŸ’¡ ChromaDB ì—°ê²° ìƒíƒœ: {'âœ… ì—°ê²°ë¨' if vector_store else 'âŒ ì—°ê²° ì•ˆë¨'}"


def handle_get_paradigm(args: dict) -> str:
    """íŒ¨ëŸ¬ë‹¤ì„ ìƒì„¸ ì •ë³´"""
    paradigm = args.get("paradigm")
    if paradigm not in PARADIGMS:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” íŒ¨ëŸ¬ë‹¤ì„: {paradigm}\nì‚¬ìš© ê°€ëŠ¥: {', '.join(PARADIGMS.keys())}"

    p = PARADIGMS[paradigm]
    return f"""## {p['name']}

### ì¡´ì¬ë¡  (Ontology)
{p['ontology']}

### ì¸ì‹ë¡  (Epistemology)
{p['epistemology']}

### ë°©ë²•ë¡  (Methodology)
{p['methodology']}

### í’ˆì§ˆ ê¸°ì¤€
{', '.join(p['quality_criteria'])}

### ì£¼ìš” í•™ì
{', '.join(p['key_scholars'])}

### í•œê³„
{p['limitations']}
"""


def handle_get_tradition(args: dict) -> str:
    """ì§ˆì ì—°êµ¬ ì „í†µ ìƒì„¸ ì •ë³´"""
    tradition = args.get("tradition")
    if tradition not in TRADITIONS:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” ì „í†µ: {tradition}\nì‚¬ìš© ê°€ëŠ¥: {', '.join(TRADITIONS.keys())}"

    t = TRADITIONS[tradition]
    variants_text = "\n".join([f"- **{k}**: {v}" for k, v in t.get("variants", {}).items()])

    return f"""## {t['name']}

### ì—°êµ¬ ì´ˆì 
{t['focus']}

### ë°ì´í„° ìˆ˜ì§‘
{t['data_collection']}

### ë¶„ì„ ë°©ë²•
{t['analysis']}

### í‘œë³¸ í¬ê¸°
{t['sample_size']}

### ì£¼ìš” í•™ì
{', '.join(t['key_scholars'])}

### ë³€í˜• (Variants)
{variants_text}
"""


def handle_suggest_methodology(args: dict) -> str:
    """ë°©ë²•ë¡  ì¶”ì²œ"""
    rq = args.get("research_question", "")
    focus = args.get("focus")

    suggestions = []

    # í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì²œ
    rq_lower = rq.lower()

    if "ê²½í—˜" in rq or "ì²´í—˜" in rq or "experience" in rq_lower:
        suggestions.append(("í˜„ìƒí•™", "ê°œì¸ì˜ ì²´í—˜ê³¼ ë³¸ì§ˆ íƒêµ¬ì— ì í•©", "phenomenology"))

    if "ê³¼ì •" in rq or "ì–´ë–»ê²Œ" in rq or "process" in rq_lower or "how" in rq_lower:
        suggestions.append(("ê·¼ê±°ì´ë¡ ", "ê³¼ì • ì„¤ëª… ë° ì´ë¡  ê°œë°œì— ì í•©", "grounded_theory"))

    if "ë¬¸í™”" in rq or "ì§‘ë‹¨" in rq or "culture" in rq_lower:
        suggestions.append(("ë¬¸í™”ê¸°ìˆ ì§€", "ë¬¸í™” ê³µìœ  ì§‘ë‹¨ ì—°êµ¬ì— ì í•©", "ethnography"))

    if "ì´ì•¼ê¸°" in rq or "ìƒì• " in rq or "story" in rq_lower or "life" in rq_lower:
        suggestions.append(("ë‚´ëŸ¬í‹°ë¸Œ íƒêµ¬", "ê°œì¸ ì´ì•¼ê¸°ì™€ ìƒì• ì‚¬ ì—°êµ¬ì— ì í•©", "narrative"))

    if "ì‚¬ë¡€" in rq or "case" in rq_lower or "ì™œ" in rq or "why" in rq_lower:
        suggestions.append(("ì‚¬ë¡€ì—°êµ¬", "ë§¥ë½ ë‚´ ì‹¬ì¸µ ë¶„ì„ì— ì í•©", "case_study"))

    if not suggestions:
        suggestions = [
            ("í˜„ìƒí•™", "ì²´í—˜ì˜ ë³¸ì§ˆ íƒêµ¬", "phenomenology"),
            ("ê·¼ê±°ì´ë¡ ", "ì´ë¡  ê°œë°œ", "grounded_theory"),
            ("ì‚¬ë¡€ì—°êµ¬", "ì‹¬ì¸µ ë¶„ì„", "case_study")
        ]

    output = f"## ì—°êµ¬ì§ˆë¬¸ ë¶„ì„\n\n**ì§ˆë¬¸**: {rq}\n\n### ì¶”ì²œ ë°©ë²•ë¡ \n\n"
    for name, reason, key in suggestions:
        t = TRADITIONS[key]
        output += f"#### {name}\n- **ì í•© ì´ìœ **: {reason}\n- **í‘œë³¸ í¬ê¸°**: {t['sample_size']}\n- **ë¶„ì„ ë°©ë²•**: {t['analysis']}\n\n"

    return output


def handle_get_coding_guide(args: dict) -> str:
    """ì½”ë”© ê°€ì´ë“œ"""
    coding_type = args.get("coding_type")
    if coding_type not in CODING_TYPES:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” ì½”ë”© ìœ í˜•: {coding_type}\nì‚¬ìš© ê°€ëŠ¥: {', '.join(CODING_TYPES.keys())}"

    c = CODING_TYPES[coding_type]
    output = f"## {c['name']}\n\n{c['description']}\n\n"

    if "process" in c:
        output += "### ì ˆì°¨\n" + "\n".join([f"- {p}" for p in c["process"]]) + "\n\n"

    if "output" in c:
        output += f"### ê²°ê³¼ë¬¼\n{c['output']}\n\n"

    if "paradigm_model" in c:
        output += "### íŒ¨ëŸ¬ë‹¤ì„ ëª¨í˜•\n"
        for k, v in c["paradigm_model"].items():
            output += f"- **{k}**: {v}\n"

    return output


def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” - ë„ì–´ì“°ê¸°, ì–¸ë”ìŠ¤ì½”ì–´ ë“±ì„ ë¬´ì‹œí•˜ê³  ë¹„êµ"""
    import re
    normalized = text.lower()
    normalized = re.sub(r'[\s_\-]', '', normalized)  # ê³µë°±, ì–¸ë”ìŠ¤ì½”ì–´, í•˜ì´í”ˆ ì œê±°
    normalized = normalized.replace('ê²€ì¦', 'ê²€í† ')  # ê²€ì¦ê³¼ ê²€í† ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    return normalized


def assess_lincoln_guba(description: str, strategies: List[str]) -> List[dict]:
    """Lincoln & Guba ê¸°ì¤€ í‰ê°€"""
    lower_desc = description.lower()
    normalized_desc = normalize_text(description)
    normalized_strategies = [normalize_text(s) for s in strategies]

    criteria = [
        {
            "criterion": "credibility",
            "korean": "ì‹ ë¹™ì„± (Credibility)",
            "strategies": [
                {
                    "name": "prolonged_engagement",
                    "korean": "ì¥ê¸°ì  ê´€ì—¬",
                    "keywords": ["ì¥ê¸°", "ì˜¤ëœê¸°ê°„", "prolonged", "7ì¼", "14ì¼", "ì§‘ì¤‘ì ê´€ì—¬", "ì§€ì†ì "]
                },
                {
                    "name": "triangulation",
                    "korean": "ì‚¼ê°í™”/ì‚¼ê°ê²€ì¦",
                    "keywords": ["ì‚¼ê°í™”", "ì‚¼ê°ê²€ì¦", "triangulation", "ë‹¤ì¤‘ìë£Œ", "3ì¤‘", "ì¸í„°ë·°+ì €ë„", "ë‹¤ì¤‘ì¶œì²˜"]
                },
                {
                    "name": "peer_debriefing",
                    "korean": "ë™ë£Œ ê²€í† ",
                    "keywords": ["ë™ë£Œê²€í† ", "ë™ë£Œê²€ì¦", "peer", "debriefing", "ë™ë£Œì—°êµ¬ì"]
                },
                {
                    "name": "member_checking",
                    "korean": "ì°¸ì—¬ì í™•ì¸",
                    "keywords": ["ì°¸ì—¬ìí™•ì¸", "membercheck", "memberchecking", "ì°¸ì—¬ìê²€í† ", "2ë‹¨ê³„í™•ì¸"]
                },
                {
                    "name": "negative_case",
                    "korean": "ë¶€ì •ì  ì‚¬ë¡€ ë¶„ì„",
                    "keywords": ["ë¶€ì •ì ì‚¬ë¡€", "negativecase", "ë°˜ì¦", "ë°©í•´ê²½í—˜", "ë¶€ì •ì‚¬ë¡€"]
                }
            ]
        },
        {
            "criterion": "transferability",
            "korean": "ì „ì´ê°€ëŠ¥ì„± (Transferability)",
            "strategies": [
                {
                    "name": "thick_description",
                    "korean": "ë‘êº¼ìš´ ê¸°ìˆ ",
                    "keywords": ["ë‘êº¼ìš´ê¸°ìˆ ", "thickdescription", "ìƒì„¸ê¸°ìˆ ", "í’ë¶€í•œê¸°ìˆ "]
                },
                {
                    "name": "purposeful_sampling",
                    "korean": "ëª©ì ì  í‘œë³¸ì¶”ì¶œ",
                    "keywords": ["ëª©ì ì ", "purposeful", "ì˜ë„ì í‘œì§‘", "ëª©ì í‘œì§‘", "ëª©ì ì í‘œë³¸", "ëª©ì í‘œë³¸"]
                },
                {
                    "name": "context_description",
                    "korean": "ë§¥ë½ ê¸°ìˆ ",
                    "keywords": ["ë§¥ë½", "context", "ë°°ê²½", "ìƒí™©ê¸°ìˆ ", "ë§¥ë½ìƒì„¸", "ë§¥ë½ì²´í¬ë¦¬ìŠ¤íŠ¸"]
                }
            ]
        },
        {
            "criterion": "dependability",
            "korean": "ì˜ì¡´ê°€ëŠ¥ì„± (Dependability)",
            "strategies": [
                {
                    "name": "audit_trail",
                    "korean": "ê°ì‚¬ ì¶”ì ",
                    "keywords": ["ê°ì‚¬ì¶”ì ", "audittrail", "ì—°êµ¬ì¼ì§€", "ê°ì‚¬ë¡œê·¸", "ì¶”ì ë¡œê·¸"]
                },
                {
                    "name": "code_recode",
                    "korean": "ì½”ë“œ-ì¬ì½”ë“œ",
                    "keywords": ["ì¬ì½”ë“œ", "recode", "ë°˜ë³µì½”ë”©", "ì½”ë“œì¬ì½”ë“œ", "ì¼ì¹˜ìœ¨", "ì½”ë”©ì¼ì¹˜"]
                },
                {
                    "name": "peer_examination",
                    "korean": "ë™ë£Œ ê²€ì¦",
                    "keywords": ["ë™ë£Œê²€ì¦", "ë™ë£Œê²€í† ", "peerexamination", "ë™ë£Œì‹¬ì‚¬"]
                }
            ]
        },
        {
            "criterion": "confirmability",
            "korean": "í™•ì¸ê°€ëŠ¥ì„± (Confirmability)",
            "strategies": [
                {
                    "name": "reflexivity",
                    "korean": "ë°˜ì„±ì„±/ì„±ì°°",
                    "keywords": ["ë°˜ì„±", "reflexiv", "ì„±ì°°", "ë°˜ì„±ì ì €ë„", "ìœ„ì¹˜ì„±", "ì €ë„ë§"]
                },
                {
                    "name": "audit_trail",
                    "korean": "ê°ì‚¬ ì¶”ì ",
                    "keywords": ["ê°ì‚¬ì¶”ì ", "audittrail", "ê°ì‚¬ë¡œê·¸"]
                },
                {
                    "name": "triangulation",
                    "korean": "ì‚¼ê°í™”/ì‚¼ê°ê²€ì¦",
                    "keywords": ["ì‚¼ê°í™”", "ì‚¼ê°ê²€ì¦", "triangulation", "3ì¤‘"]
                }
            ]
        }
    ]

    results = []
    for c in criteria:
        applied = []
        missing = []

        for strategy in c["strategies"]:
            # descriptionì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
            found_in_desc = any(
                normalize_text(k) in normalized_desc or k.lower() in lower_desc
                for k in strategy["keywords"]
            )

            # strategies_used ë°°ì—´ì—ì„œ ì°¾ê¸°
            found_in_strategies = any(
                any(normalize_text(k) in s or s in normalize_text(k) for k in strategy["keywords"])
                for s in normalized_strategies
            )

            if found_in_desc or found_in_strategies:
                applied.append(strategy["korean"])
            else:
                missing.append(strategy["korean"])

        score = round((len(applied) / len(c["strategies"])) * 25)

        results.append({
            "criterion": c["criterion"],
            "korean": c["korean"],
            "score": score,
            "max_score": 25,
            "strategies_applied": applied,
            "missing_strategies": missing,
            "recommendations": [f"{m} ì „ëµì„ ì¶”ê°€ë¡œ ì ìš©í•˜ì„¸ìš”" for m in missing] if missing else []
        })

    return results


def assess_tracy(description: str, strategies: List[str]) -> List[dict]:
    """Tracy 8ê°€ì§€ ê¸°ì¤€ í‰ê°€"""
    lower_desc = description.lower()
    normalized_desc = normalize_text(description)
    normalized_strategies = [normalize_text(s) for s in strategies]

    criteria = [
        {
            "criterion": "worthy_topic",
            "korean": "ê°€ì¹˜ìˆëŠ” ì£¼ì œ",
            "indicators": [
                "ì¤‘ìš”", "ì‹œì˜ì ì ˆ", "í•„ìš”", "ê¸°ì—¬", "ë¬¸ì œ", "ì˜ë¯¸", "ê°€ì¹˜",
                "ìƒˆë¡œìš´í˜„ìƒ", "AI", "ë¦¬ë”", "ì˜ì‚¬ê²°ì •", "íƒêµ¬", "ì—°êµ¬ëª©ì "
            ]
        },
        {
            "criterion": "rich_rigor",
            "korean": "í’ë¶€í•œ ì—„ê²©ì„±",
            "indicators": [
                "ì¶©ë¶„í•œ", "ë‹¤ì–‘í•œ", "ì ì ˆí•œ", "ì²´ê³„ì ", "ë©´ë°€í•œ", "ì—„ê²©",
                "IPA", "6ë‹¨ê³„", "ë‹¤ì¤‘ì‚¬ë¡€", "ì‹¬ì¸µ", "ë¶„ì„ì ˆì°¨", "ë¸Œë¼ì¼€íŒ…"
            ]
        },
        {
            "criterion": "sincerity",
            "korean": "ì„±ì‹¤ì„±",
            "indicators": [
                "ë°˜ì„±", "ì„±ì°°", "í•œê³„", "íˆ¬ëª…", "ì •ì§", "ìœ„ì¹˜ì„±",
                "ë°˜ì„±ì ì €ë„", "ì €ë„ë§", "ì†”ì§"
            ]
        },
        {
            "criterion": "credibility",
            "korean": "ì‹ ë¹™ì„±",
            "indicators": [
                "ì‚¼ê°", "ì°¸ì—¬ìí™•ì¸", "ë‘êº¼ìš´ê¸°ìˆ ", "êµ¬ì²´ì ", "ê²€ì¦",
                "membercheck", "ì‚¼ê°ê²€ì¦", "ë™ë£Œê²€í† "
            ]
        },
        {
            "criterion": "resonance",
            "korean": "ê³µëª…",
            "indicators": [
                "ì „ì´", "ì¼ë°˜í™”", "ë…ì", "ì˜í–¥", "ê°ë™", "ê³µê°",
                "ê²½í—˜", "ì˜ë¯¸", "ë³¸ì§ˆ", "í†µì°°"
            ]
        },
        {
            "criterion": "significant_contribution",
            "korean": "ì˜ë¯¸ìˆëŠ” ê¸°ì—¬",
            "indicators": [
                "ê¸°ì—¬", "í™•ì¥", "ìƒˆë¡œìš´", "ë°œì „", "í•¨ì˜", "ì´ë¡ ì ",
                "ì‹¤ë¬´ì ", "í†µì°°", "ì œì•ˆ"
            ]
        },
        {
            "criterion": "ethics",
            "korean": "ìœ¤ë¦¬ì„±",
            "indicators": [
                "ìœ¤ë¦¬", "ë™ì˜", "ìµëª…", "ë³´í˜¸", "IRB", "ìŠ¹ì¸",
                "ë™ì˜ì„œ", "ì² íšŒ", "ë¯¼ê°ì •ë³´", "ìµëª…í™”"
            ]
        },
        {
            "criterion": "meaningful_coherence",
            "korean": "ì˜ë¯¸ìˆëŠ” ì¼ê´€ì„±",
            "indicators": [
                "ì¼ê´€", "ì—°ê²°", "ëª©ì ", "ë°©ë²•ë¡ ", "í†µí•©", "ì í•©",
                "IPA", "í˜„ìƒí•™", "ì—°êµ¬ì§ˆë¬¸", "ë¶„ì„"
            ]
        }
    ]

    results = []
    for c in criteria:
        # descriptionê³¼ strategies ëª¨ë‘ì—ì„œ indicator ì°¾ê¸°
        found_indicators = [
            ind for ind in c["indicators"]
            if normalize_text(ind) in normalized_desc or
               ind.lower() in lower_desc or
               any(normalize_text(ind) in s for s in normalized_strategies)
        ]

        missing_indicators = [
            ind for ind in c["indicators"]
            if normalize_text(ind) not in normalized_desc and
               ind.lower() not in lower_desc and
               not any(normalize_text(ind) in s for s in normalized_strategies)
        ]

        # ì ìˆ˜ ê³„ì‚° - ìµœì†Œ 1ê°œë§Œ ë§¤ì¹˜ë˜ì–´ë„ ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬
        match_ratio = len(found_indicators) / len(c["indicators"])
        score = round(match_ratio * 13)

        results.append({
            "criterion": c["criterion"],
            "korean": c["korean"],
            "score": score,
            "max_score": 13,
            "strategies_applied": found_indicators,
            "missing_strategies": missing_indicators[:3],  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
            "recommendations": [f"{c['korean']} ê´€ë ¨ ë‚´ìš©ì„ ë³´ê°•í•˜ì„¸ìš”"] if score < 10 and len(found_indicators) < 3 else []
        })

    return results


def get_grade(percentage: float) -> str:
    """ë“±ê¸‰ ê³„ì‚°"""
    if percentage >= 90:
        return "A (ìš°ìˆ˜)"
    elif percentage >= 80:
        return "B (ì–‘í˜¸)"
    elif percentage >= 70:
        return "C (ë³´í†µ)"
    elif percentage >= 60:
        return "D (ë¯¸í¡)"
    else:
        return "F (ê°œì„  í•„ìš”)"


def get_priority_actions(assessments: List[dict]) -> List[str]:
    """ìš°ì„  ì¡°ì¹˜ ì‚¬í•­"""
    return [
        f"{a['korean']} ê°œì„ : {a['recommendations'][0] if a['recommendations'] else 'ì „ëµ ì¶”ê°€ í•„ìš”'}"
        for a in assessments
        if a['score'] / a['max_score'] < 0.5
    ][:3]


def handle_assess_quality(args: dict) -> str:
    """í’ˆì§ˆ í‰ê°€ - Lincoln & Guba + Tracy ê¸°ì¤€ìœ¼ë¡œ ì‹¤ì œ ì ìˆ˜ ì‚°ì¶œ"""
    research_description = args.get("research_description", "")
    strategies_used = args.get("strategies_used", [])
    criteria = args.get("criteria", "all")

    if not research_description:
        return "ì—°êµ¬ ì„¤ëª…(research_description)ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

    assessments = []

    if criteria == "lincoln_guba" or criteria == "all":
        assessments.extend(assess_lincoln_guba(research_description, strategies_used))

    if criteria == "tracy" or criteria == "all":
        assessments.extend(assess_tracy(research_description, strategies_used))

    # ì „ì²´ ì ìˆ˜ ê³„ì‚°
    total_score = sum(a["score"] for a in assessments)
    max_score = sum(a["max_score"] for a in assessments)
    overall_percentage = (total_score / max_score) * 100 if max_score > 0 else 0

    # ê°•ì /ì•½ì  ì‹ë³„
    strengths = [a["korean"] for a in assessments if a["score"] / a["max_score"] >= 0.7]
    weaknesses = [a["korean"] for a in assessments if a["score"] / a["max_score"] < 0.5]

    # ê²°ê³¼ êµ¬ì„±
    result = {
        "criteria_used": criteria,
        "input_summary": {
            "description_length": len(research_description),
            "strategies_reported": len(strategies_used)
        },
        "overall_assessment": {
            "score": f"{total_score}/{max_score}",
            "percentage": f"{overall_percentage:.1f}%",
            "grade": get_grade(overall_percentage)
        },
        "detailed_assessment": [
            {
                "criterion": a["criterion"],
                "korean": a["korean"],
                "score": f"{a['score']}/{a['max_score']}",
                "strategies_applied": a["strategies_applied"],
                "missing_strategies": a["missing_strategies"],
                "recommendations": a["recommendations"]
            }
            for a in assessments
        ],
        "summary": {
            "strengths": strengths,
            "weaknesses": weaknesses,
            "priority_actions": get_priority_actions(assessments)
        },
        "quality_enhancement_guide": {
            "immediate_actions": [
                "ì—°êµ¬ ì„¤ê³„ ë‹¨ê³„ì—ì„œ í’ˆì§ˆ ì „ëµì„ ê³„íší•˜ì„¸ìš”",
                "ì—°êµ¬ ì¼ì§€ë¥¼ ê¾¸ì¤€íˆ ì‘ì„±í•˜ì„¸ìš”",
                "ë™ë£Œ ì—°êµ¬ìì™€ ì •ê¸°ì ìœ¼ë¡œ í† ë¡ í•˜ì„¸ìš”"
            ],
            "during_data_collection": [
                "ì°¸ì—¬ìì™€ ì¶©ë¶„í•œ ë¼í¬ë¥¼ í˜•ì„±í•˜ì„¸ìš”",
                "ë©´ë‹´ í›„ ì¦‰ì‹œ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ì„¸ìš”",
                "ë‹¤ì–‘í•œ ìë£Œì›ì„ í™œìš©í•˜ì„¸ìš”"
            ],
            "during_analysis": [
                "ì½”ë”©ì˜ ì¼ê´€ì„±ì„ ê²€í† í•˜ì„¸ìš”",
                "ì°¸ì—¬ì í™•ì¸(member checking)ì„ ì‹¤ì‹œí•˜ì„¸ìš”",
                "ë¶€ì •ì  ì‚¬ë¡€ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì°¾ìœ¼ì„¸ìš”"
            ],
            "writing_phase": [
                "ë‘êº¼ìš´ ê¸°ìˆ ë¡œ ë§¥ë½ì„ í’ë¶€í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”",
                "ì—°êµ¬ìì˜ ìœ„ì¹˜ì„±ì„ ëª…ì‹œí•˜ì„¸ìš”",
                "í•œê³„ë¥¼ ì†”ì§í•˜ê²Œ ë…¼ì˜í•˜ì„¸ìš”"
            ]
        }
    }

    # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ê°€ë…ì„± ìˆê²Œ)
    import json
    return json.dumps(result, ensure_ascii=False, indent=2)


def handle_get_journal_guide(args: dict) -> str:
    """ì €ë„ ê°€ì´ë“œ"""
    journal = args.get("journal")
    if journal not in JOURNALS:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” ì €ë„: {journal}\nì‚¬ìš© ê°€ëŠ¥: {', '.join(JOURNALS.keys())}"

    j = JOURNALS[journal]
    output = f"## {j['name']}\n\n"
    output += f"**ì´ˆì **: {j['focus']}\n\n"
    output += f"**ìŠ¤íƒ€ì¼**: {j['style']}\n\n"

    if "key_sections" in j:
        output += "### ì£¼ìš” ì„¹ì…˜\n" + ", ".join(j['key_sections']) + "\n\n"

    if "common_rejections" in j:
        output += "### í”í•œ ë¦¬ì ì…˜ ì‚¬ìœ \n"
        for r in j['common_rejections']:
            output += f"- {r}\n"
        output += "\n"

    if "tips" in j:
        output += "### íˆ¬ê³  íŒ\n"
        for t in j['tips']:
            output += f"- {t}\n"

    return output


def handle_diagnose_rejection(args: dict) -> str:
    """ë¦¬ì ì…˜ ì§„ë‹¨"""
    rejection_type = args.get("rejection_type")
    if rejection_type not in REJECTION_PATTERNS:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” ë¦¬ì ì…˜ ìœ í˜•: {rejection_type}\nì‚¬ìš© ê°€ëŠ¥: {', '.join(REJECTION_PATTERNS.keys())}"

    r = REJECTION_PATTERNS[rejection_type]
    output = f"## {r['name']}\n\n"
    output += "### ì¦ìƒ\n" + "\n".join([f"- {s}" for s in r['symptoms']]) + "\n\n"
    output += "### í•´ê²° ì „ëµ\n" + "\n".join([f"- {s}" for s in r['solutions']])

    return output


def handle_conceptualize_idea(args: dict) -> str:
    """ì•„ì´ë””ì–´ ê°œë…í™”"""
    idea = args.get("idea", "")
    field = args.get("field", "ê²½ì˜í•™")

    return f"""## ì—°êµ¬ ì•„ì´ë””ì–´ ê°œë…í™”

### ì…ë ¥ ì•„ì´ë””ì–´
{idea}

### ë¶„ì•¼
{field}

### ê°œë…í™” í”„ë ˆì„ì›Œí¬

#### 1. í•µì‹¬ ê°œë… ì¶”ì¶œ
- ì£¼ìš” ë³€ìˆ˜/ê°œë…ì€ ë¬´ì—‡ì¸ê°€?
- ê¸°ì¡´ ë¬¸í—Œì—ì„œ ì–´ë–»ê²Œ ì •ì˜ë˜ëŠ”ê°€?

#### 2. ì´ë¡ ì  í‹€
- ì–´ë–¤ ì´ë¡ ì  ë Œì¦ˆë¡œ ë³¼ ê²ƒì¸ê°€?
- ê¸°ì¡´ ì´ë¡ ì˜ í•œê³„ëŠ”?

#### 3. ì—°êµ¬ ì§ˆë¬¸ í˜•ì„±
- ê²½í—˜ì  ì§ˆë¬¸ vs ê°œë…ì  ì§ˆë¬¸?
- What/How/Why ì¤‘ ì–´ëŠ ìœ í˜•?

#### 4. ê¸°ëŒ€ ê¸°ì—¬
- ì´ë¡ ì  ê¸°ì—¬: ìƒˆë¡œìš´ ê°œë…? ê´€ê³„? ê²½ê³„ì¡°ê±´?
- ì‹¤ë¬´ì  ê¸°ì—¬: ì–´ë–¤ ì‹œì‚¬ì ?

### ë‹¤ìŒ ë‹¨ê³„
1. í•µì‹¬ ê°œë… ì •ì˜ ë° ë¬¸í—Œ ê²€í† 
2. ì´ë¡ ì  ê¸´ì¥ ë˜ëŠ” Puzzle ì‹ë³„
3. ì—°êµ¬ ì§ˆë¬¸ ì •êµí™”
"""


def handle_develop_proposition(args: dict) -> str:
    """ëª…ì œ ê°œë°œ"""
    concept_a = args.get("concept_a", "A")
    concept_b = args.get("concept_b", "B")
    relationship = args.get("relationship", "positive")

    rel_templates = {
        "positive": f"{concept_a}ì´ ë†’ì„ìˆ˜ë¡ {concept_b}ë„ ë†’ì•„ì§„ë‹¤.",
        "negative": f"{concept_a}ì´ ë†’ì„ìˆ˜ë¡ {concept_b}ëŠ” ë‚®ì•„ì§„ë‹¤.",
        "moderation": f"{concept_a}ì™€ ì¢…ì†ë³€ìˆ˜ì˜ ê´€ê³„ëŠ” {concept_b}ì— ì˜í•´ ì¡°ì ˆëœë‹¤.",
        "mediation": f"{concept_a}ì€ {concept_b}ë¥¼ í†µí•´ ê²°ê³¼ë³€ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤."
    }

    return f"""## ì´ë¡ ì  ëª…ì œ ê°œë°œ

### ê°œë…
- **ê°œë… A**: {concept_a}
- **ê°œë… B**: {concept_b}
- **ê´€ê³„ ìœ í˜•**: {relationship}

### ëª…ì œ ì´ˆì•ˆ
**Proposition**: {rel_templates.get(relationship, f'{concept_a}ê³¼ {concept_b}ëŠ” ê´€ë ¨ì´ ìˆë‹¤.')}

### ëª…ì œ ì •êµí™” ê°€ì´ë“œ

#### 1. ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…
- ì™œ ì´ ê´€ê³„ê°€ ì¡´ì¬í•˜ëŠ”ê°€?
- ì–´ë–¤ ê³¼ì •ì„ í†µí•´ ì—°ê²°ë˜ëŠ”ê°€?

#### 2. ê²½ê³„ì¡°ê±´ ëª…ì‹œ
- ì–¸ì œ ì´ ê´€ê³„ê°€ ì„±ë¦½í•˜ëŠ”ê°€?
- ì–´ë–¤ ìƒí™©ì—ì„œ ì•½í™”/ê°•í™”ë˜ëŠ”ê°€?

#### 3. ê²½ìŸ ì„¤ëª… ê³ ë ¤
- ëŒ€ì•ˆì  ì„¤ëª…ì€ ë¬´ì—‡ì¸ê°€?
- ì™œ ê·¸ ì„¤ëª…ë³´ë‹¤ ì´ ì„¤ëª…ì´ ë‚˜ì€ê°€?

#### 4. ê²€ì¦ ê°€ëŠ¥ì„±
- ì–´ë–»ê²Œ ê²½í—˜ì ìœ¼ë¡œ ê²€ì¦í•  ìˆ˜ ìˆëŠ”ê°€?
- ì–´ë–¤ ë°ì´í„°ê°€ í•„ìš”í•œê°€?
"""


def handle_review_paper(args: dict) -> str:
    """ë…¼ë¬¸ ë¦¬ë·°"""
    section = args.get("paper_section", "")
    content = args.get("content", "")

    review_guides = {
        "introduction": """
### Introduction ê²€í†  ê¸°ì¤€

1. **Hook**: ì²« ë¬¸ì¥ì´ ì£¼ì˜ë¥¼ ë„ëŠ”ê°€?
2. **Puzzle/Gap**: ì—°êµ¬ ë¬¸ì œê°€ ëª…í™•í•œê°€?
3. **Significance**: ì™œ ì´ ì—°êµ¬ê°€ ì¤‘ìš”í•œê°€?
4. **Preview**: ì—°êµ¬ ì ‘ê·¼ë²•ì´ ì†Œê°œë˜ëŠ”ê°€?
5. **Contribution**: ê¸°ì—¬ê°€ ëª…í™•íˆ ì˜ˆê³ ë˜ëŠ”ê°€?
""",
        "literature": """
### Literature Review ê²€í†  ê¸°ì¤€

1. **Coverage**: ì£¼ìš” ë¬¸í—Œì„ í¬í•¨í•˜ëŠ”ê°€?
2. **Synthesis**: ë‹¨ìˆœ ë‚˜ì—´ì´ ì•„ë‹Œ í†µí•©ì¸ê°€?
3. **Gap Identification**: ë¬¸í—Œì˜ í•œê³„ê°€ ëª…í™•í•œê°€?
4. **Theoretical Foundation**: ì´ë¡ ì  ê¸°ë°˜ì´ ê²¬ê³ í•œê°€?
""",
        "method": """
### Method ê²€í†  ê¸°ì¤€

1. **Paradigm Fit**: ì—°êµ¬ ì§ˆë¬¸ê³¼ ë°©ë²•ë¡ ì´ ì¼ì¹˜í•˜ëŠ”ê°€?
2. **Sampling**: í‘œì§‘ ì „ëµì´ ì ì ˆí•œê°€?
3. **Data Collection**: ë°ì´í„° ìˆ˜ì§‘ì´ ì² ì €í•œê°€?
4. **Analysis**: ë¶„ì„ ì ˆì°¨ê°€ ëª…í™•í•œê°€?
5. **Rigor**: ì‹ ë¢°ì„± í™•ë³´ ì „ëµì´ ìˆëŠ”ê°€?
""",
        "findings": """
### Findings ê²€í†  ê¸°ì¤€

1. **Evidence**: ì£¼ì¥ì— ì¶©ë¶„í•œ ì¦ê±°ê°€ ìˆëŠ”ê°€?
2. **Quotes**: ì¸ìš©ì´ ì ì ˆíˆ ì‚¬ìš©ë˜ëŠ”ê°€?
3. **Organization**: êµ¬ì¡°ê°€ ë…¼ë¦¬ì ì¸ê°€?
4. **Saturation**: ì£¼ìš” ì£¼ì œê°€ í¬í™”ì— ë„ë‹¬í–ˆëŠ”ê°€?
""",
        "discussion": """
### Discussion ê²€í†  ê¸°ì¤€

1. **Interpretation**: ê²°ê³¼ í•´ì„ì´ ì ì ˆí•œê°€?
2. **Contribution**: ì´ë¡ ì  ê¸°ì—¬ê°€ ëª…í™•í•œê°€?
3. **Limitations**: í•œê³„ê°€ ì†”ì§íˆ ì¸ì •ë˜ëŠ”ê°€?
4. **Implications**: í•¨ì˜ê°€ êµ¬ì²´ì ì¸ê°€?
5. **Future Research**: í–¥í›„ ì—°êµ¬ ë°©í–¥ì´ ì œì‹œë˜ëŠ”ê°€?
"""
    }

    guide = review_guides.get(section, "ì„ íƒí•œ ì„¹ì…˜ì— ëŒ€í•œ ê°€ì´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

    return f"""## {section.upper()} ì„¹ì…˜ ë¦¬ë·°

### ê²€í†  ëŒ€ìƒ ë‚´ìš©
```
{content[:500]}{'...' if len(content) > 500 else ''}
```

{guide}

### ì¼ë°˜ í”¼ë“œë°± í”„ë ˆì„ì›Œí¬

**ê°•ì  í™•ì¸**: ì˜ ëœ ë¶€ë¶„ì€?
**ê°œì„  í•„ìš”**: ë³´ì™„ì´ í•„ìš”í•œ ë¶€ë¶„ì€?
**êµ¬ì²´ì  ì œì•ˆ**: ì–´ë–»ê²Œ ê°œì„ í•  ìˆ˜ ìˆëŠ”ê°€?
"""


def handle_guide_revision(args: dict) -> str:
    """R&R ê°€ì´ë“œ"""
    comment = args.get("reviewer_comment", "")
    comment_type = args.get("comment_type", "major")

    return f"""## R&R ìˆ˜ì • ê°€ì´ë“œ

### ë¦¬ë·°ì–´ ì½”ë©˜íŠ¸
```
{comment}
```

### ì½”ë©˜íŠ¸ ìœ í˜•
**{comment_type.upper()}**

### ëŒ€ì‘ ì „ëµ

#### 1. ì½”ë©˜íŠ¸ ë¶„ì„
- ë¦¬ë·°ì–´ê°€ ì›í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€?
- êµ¬ì²´ì  ìˆ˜ì •? ì„¤ëª… ì¶”ê°€? ë°ì´í„° ë³´ê°•?

#### 2. ëŒ€ì‘ ì˜µì…˜
- **ìˆ˜ìš©**: ì½”ë©˜íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜
- **ë¶€ë¶„ ìˆ˜ìš©**: ì¼ë¶€ë§Œ ë°˜ì˜í•˜ê³  ì´ìœ  ì„¤ëª…
- **ë°˜ë°•**: ì •ì¤‘í•˜ê²Œ ë°˜ë¡  (ê·¼ê±° í•„ìˆ˜)

#### 3. ì‘ë‹µ ì‘ì„±
- ê°ì‚¬ í‘œí˜„ìœ¼ë¡œ ì‹œì‘
- êµ¬ì²´ì  ìˆ˜ì • ë‚´ìš© ëª…ì‹œ
- í˜ì´ì§€/ë¼ì¸ ë²ˆí˜¸ í¬í•¨

#### 4. ìˆ˜ì • íŒ ({comment_type})
{"- ì‹ ì¤‘í•˜ê³  ì² ì €í•œ ìˆ˜ì • í•„ìš”\n- ì¶”ê°€ ë¶„ì„ì´ë‚˜ ë°ì´í„° ë³´ê°• ê³ ë ¤\n- ì´ë¡ ì  ë…¼ê±° ê°•í™”" if comment_type == "major" else "- ê°„ë‹¨í•œ ìˆ˜ì •ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥\n- ëª…í™•í•œ ì„¤ëª… ì¶”ê°€" if comment_type == "minor" else "- ì„¤ëª…ë§Œ ì¶”ê°€í•˜ë©´ ë¨\n- ë³¸ë¬¸ ìˆ˜ì • ì—†ì´ í•´ëª… ê°€ëŠ¥"}

### ì‘ë‹µ í…œí”Œë¦¿
```
We thank the reviewer for this valuable comment. [ê°ì‚¬]

[êµ¬ì²´ì  ëŒ€ì‘ ë‚´ìš©]

We have revised the manuscript accordingly. Please see [section/page] for the updated version.
```
"""


# ============================================================================
# Main Tool Handler
# ============================================================================

async def handle_tool_call(name: str, arguments: dict) -> dict:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    handlers = {
        "search_knowledge": handle_search_knowledge,
        "get_paradigm": handle_get_paradigm,
        "get_tradition": handle_get_tradition,
        "suggest_methodology": handle_suggest_methodology,
        "get_coding_guide": handle_get_coding_guide,
        "assess_quality": handle_assess_quality,
        "get_journal_guide": handle_get_journal_guide,
        "diagnose_rejection": handle_diagnose_rejection,
        "conceptualize_idea": handle_conceptualize_idea,
        "develop_proposition": handle_develop_proposition,
        "review_paper": handle_review_paper,
        "guide_revision": handle_guide_revision
    }

    handler = handlers.get(name)
    if handler:
        result = handler(arguments)
        return {"content": [{"type": "text", "text": result}]}
    else:
        return {"content": [{"type": "text", "text": f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"}], "isError": True}


# ============================================================================
# FastAPI Application
# ============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("=" * 50)
    logger.info("GPT QualMaster MCP Server Starting")
    logger.info("12 tools available for qualitative research")

    # Initialize ChromaDB
    if init_chromadb():
        logger.info("âœ… ChromaDB RAG search enabled")
    else:
        logger.warning("âš ï¸ ChromaDB not available - using embedded knowledge only")

    logger.info("=" * 50)
    yield
    logger.info("Server shutting down")


app = FastAPI(
    title="GPT QualMaster MCP",
    description="AI-Powered Qualitative Research & Conceptual Paper Writing Assistant",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
async def root():
    return {
        "status": "running",
        "server": SERVER_INFO,
        "tools_count": len(TOOLS)
    }


@app.get("/health")
async def health():
    return {"status": "healthy", "tools": len(TOOLS)}


@app.get("/mcp")
async def mcp_sse_endpoint(request: Request):
    """SSE endpoint for GPT MCP connections"""
    # Get the base URL from the request
    host = request.headers.get("host", "localhost:8780")
    scheme = request.headers.get("x-forwarded-proto", "http")
    base_url = f"{scheme}://{host}"

    async def event_generator():
        # First, send the endpoint event (MCP SSE protocol requirement)
        yield f"event: endpoint\ndata: {base_url}/mcp\n\n"

        # Send server info as a message
        init_event = {
            "jsonrpc": "2.0",
            "result": {
                "protocolVersion": "2024-11-05",
                "serverInfo": SERVER_INFO,
                "capabilities": {"tools": {}}
            },
            "id": 0
        }
        yield f"event: message\ndata: {json.dumps(init_event)}\n\n"

        # Keep connection alive
        while True:
            await asyncio.sleep(30)
            yield f": keepalive\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@app.post("/mcp")
async def mcp_endpoint(request: Request):
    try:
        body = await request.json()

        if body.get("method") == "initialize":
            return JSONResponse({
                "jsonrpc": "2.0",
                "result": {
                    "protocolVersion": "2024-11-05",
                    "serverInfo": SERVER_INFO,
                    "capabilities": {"tools": {}}
                },
                "id": body.get("id")
            })

        elif body.get("method") == "tools/list":
            return JSONResponse({
                "jsonrpc": "2.0",
                "result": {"tools": TOOLS},
                "id": body.get("id")
            })

        elif body.get("method") == "tools/call":
            params = body.get("params", {})
            tool_name = params.get("name", "")
            # Defensive coding: argumentsê°€ Noneì´ê±°ë‚˜ ì—†ëŠ” ê²½ìš° ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì²˜ë¦¬
            arguments = params.get("arguments")
            if arguments is None or not isinstance(arguments, dict):
                arguments = {}
            result = await handle_tool_call(tool_name, arguments)
            return JSONResponse({
                "jsonrpc": "2.0",
                "result": result,
                "id": body.get("id")
            })

        else:
            return JSONResponse({
                "jsonrpc": "2.0",
                "error": {"code": -32601, "message": "Method not found"},
                "id": body.get("id")
            })

    except Exception as e:
        logger.error(f"Error: {e}")
        return JSONResponse({
            "jsonrpc": "2.0",
            "error": {"code": -32603, "message": str(e)},
            "id": None
        }, status_code=400)


def main():
    print("\n" + "=" * 60)
    print("  GPT QualMaster MCP Server v1.0.0")
    print("  AI-Powered Qualitative Research Assistant")
    print("=" * 60)
    print("  URL: http://127.0.0.1:8780")
    print("  ngrok: ngrok http 8780")
    print("-" * 60)
    print("  12 Tools:")
    for t in TOOLS:
        print(f"    - {t['name']}")
    print("=" * 60 + "\n")

    uvicorn.run(app, host="127.0.0.1", port=8780, log_level="info")


if __name__ == "__main__":
    main()
